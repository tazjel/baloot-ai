import os
import sys
import json
import logging

# Setup Paths
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from server.common import db
# Ensure tables are defined
import server.models

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("BenchmarkGenerator")

def generate_benchmark():
    """
    Extracts 'Golden' training examples from the database and saves them 
    as a standardized Benchmark Set (Puzzles).
    """
    try:
        # Fetch training data that has a "correct_move" (Generated by Gemini)
        # We assume any row in bot_training_data is a "correction" of a mistake, 
        # so it's a perfect puzzle: "Bot did X, but should have done Y".
        rows = db(db.bot_training_data).select()
        
        puzzles = []
        for row in rows:
            try:
                state = json.loads(row.game_state_json)
                correct_move = json.loads(row.correct_move_json)
                
                # Validate structure (Must be a playable game state)
                players = state.get('players', [])
                if not state or not correct_move or len(players) != 4: 
                    # logger.warning(f"Skipping row {row.id}: Invalid Structure (Players={len(players)})")
                    continue
                
                puzzle = {
                    "id": f"puzzle_{row.id}",
                    "context_hash": row.context_hash,
                    "description": row.reason or "Find the best move.",
                    "game_state": state,
                    "solution": correct_move,
                    "difficulty": "Hard" # All mistakes are hard by definition
                }
                puzzles.append(puzzle)
            except Exception as e:
                logger.warning(f"Skipping row {row.id}: {e}")
                
        logger.info(f"Extracted {len(puzzles)} puzzles from database.")
        
        # Save to file
        output_path = os.path.join("ai_worker", "benchmarks", "golden_puzzles.json")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(puzzles, f, indent=2)
            
        logger.info(f"Saved benchmark set to {output_path}")
        print(f"âœ… Generated {len(puzzles)} puzzles in {output_path}")
        
    except Exception as e:
        logger.error(f"Failed to generate benchmark: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    generate_benchmark()
